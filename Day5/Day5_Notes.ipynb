{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Day5_Notes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewFried/Udemy/blob/master/Day5/Day5_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvXWU1AkONB5"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# Day 5: Regression Modeling and Assessment\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0wzopBcONB9"
      },
      "source": [
        "## Day4 Review\n",
        "\n",
        "### A Simple Method for Calculating a ROC Curve\n",
        "\n",
        "A ROC curve is formulated by plotting the __true positive rate__ (TPR) (a.k.a., __sensitivity__) against the __false positive rate__ (FPR) (a.k.a, __1 - specificity)__ for a given classifier's actual classifications and scored probabilities. \n",
        "\n",
        "The FPR is represented by the X axis while the TPR is represented by the Y axis.\n",
        "\n",
        "__Remember__:  \n",
        "\n",
        "- The __true positive rate__ (TPR) is the proportion of actual \"positive\" classification values that are __correctly identified as \"positive\"__. This is also known as the __sensitivity__ of a classifier, i.e., TP / (TP + FN)\n",
        "\n",
        "\n",
        "- The __false positive rate__ (FPR) is the proportion of actual \"negative\" classification values that are __incorrectly identified as \"positive\"__. FPR is defined as FP / (FP + TN), which is the same as __1 - specificity__ (remember that specificity = TN/(TN + FP) )\n",
        "\n",
        "\n",
        "To derive the values required for a ROC plot of a model we __iterate through an ordered sequence of 'K' classification threshold values__ within the range of $(0 < threshold <= 1)$ (e.g., if 'K' == 100, our iterator would span .01 through 1.00 via .01 increments). \n",
        "\n",
        "While iterating through the threshold values, we __compare the scored probabilities against each iterative threshold value__ to derive + store TPR and FPR values across the full range of iterable threshold values. When finished, we have accumulated 'K' pairs of TPR and FPR values.\n",
        "\n",
        "Then, plot the resulting TPR and FPR metrics against one another on a 2-dimensional plane.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-PCsdgLONB-"
      },
      "source": [
        "''' 'actual' is the class variable (the actual classification) from the data set\n",
        "'probability' is the scored.probability variable from the data set '''\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "    #create list of thresholds\n",
        "    thresholds = np.arange(0,1,0.01)\n",
        "    \n",
        "    #create list of true positive rates\n",
        "    tpr_list=[]\n",
        "    \n",
        "    #create list of false positive rates\n",
        "    fpr_list=[]\n",
        "    \n",
        "    #create new list of predictions for every threshold\n",
        "    for i in thresholds: #for every threshold\n",
        "        new_predicted=[] #create new list of predictions\n",
        "        for j in range(len(probability)): #for each observation\n",
        "            #compare threshold to probability and append new prediction\n",
        "            if probability[j] < i:        \n",
        "                new_predicted.append(0)\n",
        "            else:\n",
        "                new_predicted.append(1)\n",
        " \n",
        "        #calculate the tpr\n",
        "        tpr = sensitivity(actual, pd.Series(new_predicted))\n",
        "        tpr_list.append(tpr) #append the true positive rate to the list\n",
        "\n",
        "        #calculate the fpr\n",
        "        fpr = 1 - specificity(actual, pd.Series(new_predicted))\n",
        "        fpr_list.append(fpr) #append the false positive rate to the list\n",
        "        \n",
        "# ...then continue to formulate your code for plotting tpr_list vs. fpr_list\n",
        "# via the Python graphics tool of your choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMzMRXq2ONB-"
      },
      "source": [
        "### Calculating AUC\n",
        "\n",
        "Once you have derived the data values required for plotting a ROC curve, calculating the area under that curve is a simple matter of applied mathematics / calculus. There are a variety of mathematical approaches for estimating the area under a curve, with two of the most prominent being the trapezoid rule or Reimann sums."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO70kpRmONB_"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# Regression Modeling for Numeric Response Variables\n",
        "\n",
        "__Regression modeling__ estimates the strength of the relationship between the __\"Response\" (aka \"Dependent\") variable__ and one or more __\"Explanatory\" (aka \"Independent\") variables__. \n",
        "\n",
        "It is used for __predicting the mean numerical value of the response variable__ when given specific values for the explanatory variables.\n",
        "\n",
        "There are many different types of regression models, and the characteristics of each vary depending on the type of response variable you are attempting to estimate. Some of the most widely used types of regression models for __numerical response variables__ include:\n",
        "\n",
        "- __Linear Regression__: Used for fitting a linear equation for a __continuous__ numeric response variable. The relationship between the response and explanatory variable(s) is assumed to be __linear__ in nature. The output is a __linear equation__. If there is only one explanatory variable, we use __Simple Linear Regression__. If there is more than one explanatory variable, we use __Multiple Linear Regression__.\n",
        "\n",
        "\n",
        "- __\"Count\" Regression__: The response variable contains __non-negative__ (i.e., x >= 0) __discrete__ numeric \"count\" values while the explanatory variables can be either binary, discrete or continuous. Commonly used regression techniques for modeling count data include __Poisson Regression__ (where the response variable __must__ have a Poisson distribution), __Negative Binomial Regression__ (where the response variable need not have a Poisson distribution), and __Zero-Inflated Negative Binomial Regression__ (used when the response variable contains an excessive number of 'zero' values).\n",
        "\n",
        "\n",
        "- __Polynomial Regression__: Used when a __non-linear__ relationship exists between a continuous response variable and explanatory variables that are either binary, discrete or continuous. The ouput is a __non-linear__ equation.\n",
        "\n",
        "\n",
        "\\*\\* Please note that there are __many__ other types of regression models (e.g., Lasso, Ridge, Partial Least Squares, etc).\\*\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdzmxjswONB_"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "Linear regression consists of finding the best fitting straight line through a set of data points. That line is called a __regression line__. An example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m55WmP9ONCA"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-JfWFxEONCA",
        "outputId": "f55a79fe-d8b3-4a07-e983-27d084da0afd"
      },
      "source": [
        "# generate some random data to use for the plot\n",
        "x = np.random.randn(50) * 10\n",
        "\n",
        "y = np.random.randn(50) * 10\n",
        "\n",
        "# first define a Matplotlib figure\n",
        "plt.figure()\n",
        "\n",
        "# use Seaborn's regplot() function to plot the regression line for the \n",
        "sns.regplot(x, y)\n",
        "\n",
        "# give the plot a title using Matplotlib\n",
        "plt.title('Regression Plot');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2D28Np8ONCB"
      },
      "source": [
        "Simple linear regression equations take the form of \n",
        "$y' = ax + b$ where $y'$ is the predicted value of the response variable for a given value of the explanatory variable $x$, $a$ is the slope of the regression line, $x$ is the value of the explanatory variable and $b$ is a constant representing the y-intercept of the regression line. \n",
        "\n",
        "The regression line is created by using the known values of the explanatory variable $x$ and the known values of the response variable $y$ to find the slope $a$ and y-intercept $b$ of the line that minimizes the sum of the squared errors of the predicted values of $y'$. \n",
        "\n",
        "We calculate values for the slope $a$ and the y-intercept $b$ using the means and standard deviations of the known values of $x$ and $y$ as well as the correlation coefficient $r$ of $x$ and $y$:\n",
        "\n",
        "$a = r * stddev(y)/stddev(x) $\n",
        "\n",
        "$b = f(x=0)$ that is, the expected y when x is 0\n",
        "\n",
        "\n",
        "A simple linear regression example:\n",
        "\n",
        "http://onlinestatbook.com/2/regression/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTruEUNHONCB"
      },
      "source": [
        "## Multiple Linear Regression\n",
        "\n",
        "When using multiple explanatory variables, the regression line __cannot be visualized within a two-dimensional space__. However, it is still relatively easy to compute: the regression line is estimated using the formula\n",
        "\n",
        "$y' = a + b1x1 + b2x2 + .. +bnxn$\n",
        "\n",
        "\n",
        "where $y'$ is the predicted value for the response variable for given values of the explanatory variables x1, x2, .., xn, and the $b$ values represent the __independent contributions__ of the corresponding explanatory variable to the prediction of the response variable.  \n",
        "\n",
        "For those who desire further explanation of the underlying mathematics:\n",
        "\n",
        "http://www.statsoft.com/Textbook/Multiple-Regression\n",
        "\n",
        "http://faculty.cas.usf.edu/mbrannick/regression/Reg2IV.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZyzLUaONCB"
      },
      "source": [
        "## Count Regression\n",
        "\n",
        "Unlike linear regression, __count regression__ is appropriate for models in which the response variable is a __non-negative integer__.  The response variable should follow either a __Poisson distribution__ (i.e., mean == variance) or an __overdispersed distribution__ (i.e., variance > mean).\n",
        "\n",
        "\n",
        "#### Why not just use some form of linear regression for estimating non-negative integers?\n",
        "\n",
        "-  The relationship between the explanatory variables and the non-negative integer response variable __are unlikely to be linear__.\n",
        "\n",
        "\n",
        "-  Linear regression can produce __negative floating point numbers__, which are clearly not valid if the response variable is known to be a non-negative integer.\n",
        "\n",
        "\n",
        "-  The distribution of the residuals from a linear regression model that has been applied to a non-negative integer response variable __will not be random__.\n",
        "\n",
        "\n",
        "### Poisson Regression\n",
        "\n",
        "If your data follows a Poisson distribution, __Poisson Regression__ can be an effective tool for estimating the non-negative integer response variable. The probability mass function (PMF) for a Poisson distribution is:\n",
        "\n",
        "# $p(X = k) =  \\frac{\\lambda^k e^{-\\lambda}} {k!}$\n",
        "\n",
        "where $k$ is any non-negative integer, $e$ is the natural exponent $e = 2.71828$, and $\\lambda$ is the mean value of the response variable.  (Note that the mean is equivalent to the variance in a Poisson distribution). Using the PMF we can estimate the likelihood of any value within a Poisson distribution. \n",
        "\n",
        "Poisson regression incorporates this PMF as it calculates a __Maximum Likelihood Estimation__ for the regression coefficients that maximize the likelihoods of the individual response variable values. \n",
        "\n",
        "The output of the algorithm will be the __natural log__ of the count value you are attempting to estimate. Therefore, you must __exponentiate__ the output of the model to determine the actual estimated values for the count variable. \n",
        "\n",
        "__How to interpret the model's coefficients__: \"For a one unit change in $X_k$, the estimated count changes by a factor of $exp(\\beta_k)$, assuming all other variables are held constant.\" However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "\n",
        "See this article from the assigned readings for more detail on the underlying mathematics + an example of how to implement Poisson regression in Python: https://towardsdatascience.com/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958\n",
        "\n",
        "\n",
        "### Negative Binomial Regression\n",
        "\n",
        "If your data is __over dispersed__ (i.e, variance > mean), __Negative Binomial Regression__ can be an effective tool for estimating the non-negative integer response variable. The probability mass function (PMF) for a Negative Binomial (\"overdispersed\") distribution is:\n",
        "\n",
        "# $p(X = n) =  \\frac{(n-1)!} {(r-1)!(n-r)!}p^r (1-p)^{(n-r)}$\n",
        "\n",
        "where $n$ is the number of attempts, $r$ is the number of successes, and $p$ is the probability of success. For our purposes \"probability of success\" represents the likelihood that the response variable has been observed having a specific value (e.g., 0 or 1 or 2 or ...).\n",
        "\n",
        "Negative Binomial regression incorporates this PMF as it calculates a __Maximum Likelihood Estimation__ for the regression coefficients that maximize the likelihoods of the individual response variable values.\n",
        "\n",
        "As with Poisson regression, the output of the negative binomial regression algorithm will be the __natural log__ of the count value you are attempting to estimate. Therefore, you must __exponentiate__ the output of the model to determine the actual estimated value for the count variable. \n",
        "\n",
        "__How to interpret the model's coefficients__: Interpretation of the explanatory variable coefficients is identical to Poisson regression: \"For a one unit change in $X_k$, the estimated count changes by a factor of $exp(\\beta_k)$, assuming all other variables are held constant.\" However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "See this article from the assigned readings for an explanation of how to implement negative binomial regression in Python: https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4\n",
        "\n",
        "\n",
        "See this article from the assigned readings for an explanation of how to implement __both__ Poisson and Negative Binomial Regression via __Generalized Linear Models__ (GLM's: https://en.wikipedia.org/wiki/Generalized_linear_model) using Python's __statsmodels__ library: \n",
        "\n",
        "\n",
        "- https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/#cameron\n",
        "\n",
        "\n",
        "This article provides a detailed tutorial of how to implement both Poisson and Negative Binomial regression using Generalized Linear Models (GLM’s) via Python's statsmodels library. __It is strongly encouraged to use this tutorial when implementing a count regression model within Python__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QWa1SahONCC"
      },
      "source": [
        "## Polynomial Regression\n",
        "\n",
        "When a __non-linear__ relationship exists between a continuous response variable and explanatory variables that are either binary, discrete or continuous, a linear regression model is highly unlikely to be an effective model. As an alternative, we can substitute a __polynomial equation__ for the standard linear model. The ouput of such a regression model is a __non-linear__ equation.\n",
        "\n",
        "__What is a non-linear equation?__ An example:\n",
        "\n",
        "# $y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_dx_i^d + \\epsilon_i $\n",
        "\n",
        "As $d$ (the degree of the polynomial) increases, the \"flexibility\" of the regression model increases significantly. See the assigned reading for an example: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n",
        "\n",
        "__HOWEVER__, this flexibility comes at a cost: we can easily end up with a model having very high variance (i.e., the model __overfits__ the data). A model with very high variance is unlikely to prove to be effective when applied to previously unseen data. \n",
        "\n",
        "__THEREFORE__: when using polynomial regression we need to be acutely aware of the __bias vs variance__ tradeoff and work diligently to select a polynomial degree $d$ that strikes a balance between the two.\n",
        "\n",
        "__It is unusual to use a $d$ of more than 3 or 4__ since using larger values can easily result in overfitting. However, there is no strict limitation on the selection of a value for $d$ so use your empirical skills to help you decide on a value for $d$ that is best suited to the data you are working with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzo-MGG_ONCC"
      },
      "source": [
        "## Evaluating Regression Model Performance\n",
        "\n",
        "In __Module 5__ we discussed the following regression model performance metrics:\n",
        "\n",
        "- $R^2$: Often referred to as \"Goodness of fit\"; measures how much of the variation in the response variable $y'$ is explained by variation in the explanatory variable(s). In general, __the larger the value of $R^2$, the more accurate the model is__. However, relatively large values of $R^2$ can also be an indication of the model being \"overfit\" to the training data.\n",
        "\n",
        "\n",
        "- __Adjusted__ $R^2$: Increases if a new variable added to the model improves the fit of the model by more than would be expected by sheer chance. When comparing two models derived from the same data, models with __higher__ __Adjusted__ $R^2$ scores are preferable to those having relatively lower scores. \n",
        "\n",
        "\n",
        "- __AIC__: Akaike Information Criteria is a model selection metric that estimates the relative quality of a statistical model for a given set of data. When comparing two models derived from the same data, models with __lower__ __AIC__ scores are preferable to those having relatively higher scores.\n",
        "\n",
        "\n",
        "- __BIC__: Bayesian Information Criteria is another model selection metric that estimates the unexplained variation in the response variable relative to the given explanatory variables. __BIC__ also imposes a \"complexity\" penalty when the number of explanatory variables used is increased.  When comparing two models derived from the same data, models with __lower__ __BIC__ scores are preferable to those having relatively higher scores.\n",
        "\n",
        "\n",
        "- __F Statistic__: Indicates whether a significant amount of variance in the response variable $y'$ is explained by the model. When comparing two models derived from the same data, models with __higher__ __F Statistic__ scores are preferable to those having relatively lower scores. \n",
        "\n",
        "\n",
        "- __Log Likelihood__: A measure of how well a model fits the underlying data. When comparing two models derived from the same data, models with __higher__ __Log Likelihood__ scores are preferable to those having relatively lower scores. \n",
        "\n",
        "\n",
        "- __p values__: Measure the statistical significance of the explanatory variables in your model. While you are free to select the significance level on your own, most often 0.05 is used as the maximum bound for significance. As such, if any variable in your model is shown to have a __p value__ that exceeds 0.05, consider removing it from the model to see whether the fit/model selection metrics improve.\n",
        "\n",
        "\n",
        "- __Root Mean Squared Error (RMSE)__: Average distance of a sample from its observed value to its predicted value. We calculate the RMSE by finding the square root of the average of the squared values of a model's residual values. When comparing two models derived from the same data, models with __lower__ __RMSE__ scores are preferable to those having relatively higher RMSE scores. \n",
        "\n",
        "\n",
        "These metrics allow us to assess the efficacy of our regression models. Practitioners should examine such metrics whenever they are readily available (e.g., if the software being used generates them automatically). If such metrics are not readily available as output from a model you are testing, you should consider whether your work requires you to calculate them via some other method as a separate step during your model evaluation work.\n",
        "\n",
        "\n",
        "### Residual Plots & Examining the \"Normality\" of Residuals\n",
        "\n",
        "In addition to the metrics listed above, we can also make use of the __residuals__ of a regression model to help us determine whether or not our models are robust. \n",
        "\n",
        "A __residual__ is the difference between the actual value of a response variable for a given observation and the predicted response variable value for that same observation. \n",
        "\n",
        "In general, the residuals for __linear regression models__ should be __normally distributed__ and __homoscedastic__ (i.e., their distribution is random across all estimated values and no pattern is emminently obvious). We can assess the residuals of a linear regression model via various types of plots. From your assigned readings (MLPR Ch 15): https://github.com/mattharrison/ml_pocket_reference/blob/master/ch15.ipynb\n",
        "\n",
        "Note that non-linear models are unlikely to have normally distributed residuals; as such, testing for the normality of residuals of such models is unnecessary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8nwsAQbONCC"
      },
      "source": [
        "# Day5 Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2XqgkVSV3Q"
      },
      "source": [
        "We will be building on the previously done wine data set. Our previous data set: [here](https://raw.githubusercontent.com/MatthewFried/Udemy/master/Day2/Day2_Data.csv) will be applied to our new set: [here](https://github.com/MatthewFried/Udemy/blob/master/Day5/Day5_Data.csv).\r\n",
        "\r\n",
        "We are looking to model the data set's TARGET attribute, which represents the number of cases of wine that were  purchased by wine distributors subsequent to their sampling each of the wines. Sample cases of wine are used  to provide tasting samples to wine shops and restaurants throughout the USA. Create a model predicts the number of wine cases ordered by  distributors based on the various characteristics of the many wines represented in the data set. The wine  producer is interested in understanding ways in which their own wine offerings can be adjusted to maximize  wine sales. \r\n",
        "\r\n",
        "Construct and compare/contrast a series of regression models (after completing the  necessary EDA and data prep work) that predict the number of wine cases sold relative to certain  properties/characteristics of the wine.\r\n",
        "\r\n",
        "* Upload the data and store it in a dataframe\r\n",
        "\r\n",
        "* Perform an EDA and any necessary data preparation work\r\n",
        "\r\n",
        "* Use feature selection and/or dimensionality reduction techniques to identify  explanatory variables for inclusion within your models. You may select the features manually via the  application of domain knowledge, use forward or backward selection, or use a different feature  selection method (e.g., decision trees, etc.)\r\n",
        "\r\n",
        "* Construct at least two different Poisson regression models, at least two different negative binomial regression models, and at least two multiple linear regression models,  using different explanatory variables (or the same variables if they have been transformed via different  transformation methods). At times, Poisson and negative binomial models can produce identical  results. Be sure to comment on that if it happens.  \r\n",
        "\r\n",
        "* After training your various models, decide how you will select the “best” regression model from those  you have constructed. For example, are you willing to select a model with slightly lower performance if  it is easier to interpret or less complicated to implement? What metrics will you use to  compare/contrast your models? Evaluate the performance of your models via cross validation using  the training data set. Then apply your preferred model to the evaluation data set and assess how well  it performs on that previously unseen data. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0GG5_w8kOG1"
      },
      "source": [
        "## **AFTER - Review** \r\n",
        "\r\n",
        "#### Outliers\r\n",
        "\r\n",
        "- __Is my approach to identifying outliers appropriate for each of the attributes__? Research / develop to gain domain knowledge relative to the data you are working with so that you are well-informed regarding the possible __valid__ data values for each of your attributes.\r\n",
        "\r\n",
        "- __If I remove observations, am I discarding too many observations__ ? If your outlier analysis results in the discarding of a large percentage of the observations contained within a data set, make sure you have a strong set of supporting facts/ domain knowledge that justify your actions.\r\n",
        "\r\n",
        "- __Is there some other method (other than deletion) I can make use of when I have identified a likely outlier__? Think about why the supposed outlying values may be present within the data set: Are they actually valid values? Could they be the result of a data entry or computational error of some sort? Are there similar observations within the data set? Can we preserve otherwise valid data via the use of a well-founded imputation of a new value for the supposed outlier? etc.\r\n",
        "\r\n",
        "- __Is there a legitamite reason the data is missing?__  For example, the STARS variable had a significant number of \"missing\" data values. However, use of domain knowledge would tell us that not every wine available for sale actually has a STARS rating: only wines that have been reviewed by so-called \"experts\" who are qualified to assign STARS ratings will actually receive a STARS rating. Therefore, the lack of a STARS rating is __very__ likely to be indicative of the fact that a wine simply has not been assessed for a STARS rating as of yet, and the lack of a STARS value is, in fact, __valid__ within the context of this data set.\r\n",
        "\r\n",
        "<br>\r\n",
        "</br>\r\n",
        "\r\n",
        "#### Imputation\r\n",
        "\r\n",
        "Avoid mean, median, or mode for imputation (if you can) since it will likely alter the shape of the probability density function of an attribute __unless the number of missing values is very small__. Use of a mean or median or mode should be reserved solely for instances of when no other method proves viable (e.g., no statistically significant imputation model can be identified for the attribute) or when we have a very small number of missing data values (e.g., less than 1%).\r\n",
        "\r\n",
        "<br>\r\n",
        "</br>\r\n",
        "\r\n",
        "#### Discuss + explain your model coefficients\r\n",
        "\r\n",
        "The \"directionality\" of model coefficients can provide a great deal of information we can use to help explain a model to others. Therefore, we should __always__ review and discuss the directionality of our model coefficients for purposes of explaining to others the effects that various explanatory variables have upon our response variable.\r\n",
        "\r\n",
        "<br>\r\n",
        "</br>\r\n",
        "\r\n",
        "#### The Actual Analysis\r\n",
        "\r\n",
        "The response variable is actually __under-dispersed__ due to all of the zeroes contained within the TARGET variable. When data is underdispersed AND we have a large volume of zeroes in our response variable, __a zero-inflated model is likely to be the most effective type of model to use__. As such, we could apply a zero-inflated Poisson or zero-inflated negative binomial model to the data (NOTE: Python lacks a reliable zero-inflated negative binomial algorithm. However, a ready-made zero-inflated Poisson algorithm can be found [here](https://austinrochford.com/posts/2015-03-03-mle-python-statsmodels.html ).\r\n",
        "\r\n",
        "<br>\r\n",
        "</br>\r\n",
        "\r\n",
        "#### Linear regression\r\n",
        "\r\n",
        "__Linear regression models can generate negative non-whole values__. Since our response variable was a cardinal non-negative integer, use of a linear regression model would not be appropriate for purposes of estimating the number of cases of wine likely to be sold."
      ]
    }
  ]
}