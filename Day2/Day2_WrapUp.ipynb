{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2_WrapUp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbABmABT2i9u",
        "colab_type": "text"
      },
      "source": [
        "#Day2 Wrap-Up\n",
        "\n",
        "* **Missing Values**: Only 6436 of 12795 rows (50.3%) represent \"complete cases\", the rest are missing data. Analysis shows that eight of the fourteen predictor variables have missing data values.\n",
        "\n",
        "* **Stars Category**: More than 26% of STARS values are missing. Is that necessarily a cause for concern? Based on domain knowledge + common sense, are there any potentially plausible explanations for why so many wines lack a STARS value? Yes, because they simply haven't been tested yet.\n",
        "\n",
        "* __Negative Values__: Nine variables representing various chemical composition measures within wines (Chlorides, ResidualSugar, FreeSulfurDioxide, CitricAcid, VolatileAcidity, TotalSulfur, DioxideSulphates, FixedAcidity and Alcohol) contain large numbers of negative data values. Further analysis of the negative values reveals that they are pervasive throughout the data set and seem to occur randomly within the 12795 observations.  A cursory survey of the typically reported values for each of these nine attributes reveals that their values __should be strictly positive__ and __zero bound__.  In other words, __none of these variables could plausibly contain a valid negative value within the context of our data set__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrnwooO4rKy",
        "colab_type": "text"
      },
      "source": [
        "## Reasonable Assumptions and Approaches\n",
        "\n",
        "###__Negative Values__ \n",
        "\n",
        "Since we see that negative values are implausible, here are some possible approaches: \n",
        "- Add a constant (e.g., the absolute value of the minimum value for a given variable) to each negative value to ensure that all values within an atrribute are positive.\n",
        "\n",
        "- Use the absolute value of each negative data value in place of the negative value.\n",
        "\n",
        "- Simply delete the attributes containing the negative data values\n",
        "\n",
        "- Delete any observations containing negative data values\n",
        "\n",
        "### __Missing Data Values__\n",
        "\n",
        "Since we have to deal with the missing values and dropping half the data isn't plausible, here are some approaches: \n",
        "\n",
        "- Use of mean, median or mode values\n",
        "\n",
        "- Imputation via a K-Nearest Neighbors imputation (e.g., https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)\n",
        "\n",
        "- Imputation via backfilling and forward filling\n",
        "\n",
        "- Imputation via an iterative imputer (i.e., each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features. For an example see: https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/ )\n",
        "\n",
        "An imputation method choice can depend on computation complexity and the reliability / performance of each potential approach. For example, for a very small number of missing values it might be appropriate to rely on the use of a median value since imputing the same value for a small number of instances is relatively unlikely to introduce bias within your data (i.e., __alter the probability density function derived from the variable's known data__). \n",
        "\n",
        "For situations in which we have more than just a small number of missing values, use of a more complex imputation approach is generally warranted to increase the likelihood that we will maintain the shape of the probability density function we derived from the variable's known data."
      ]
    }
  ]
}