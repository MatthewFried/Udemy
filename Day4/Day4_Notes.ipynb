{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Day4_Notes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewFried/Udemy/blob/master/Day4_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYqWDCugBiC0"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# Day4: Evaluating Machine Learning Model Performance\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnZi8B6qBiC2"
      },
      "source": [
        "# Evaluating Machine Learning Model Performance\n",
        "\n",
        "\n",
        "## Bias vs. Variance\n",
        "\n",
        "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
        "\n",
        "Imagine we can repeat the entire model building process multiple times: each time we gather new data and run a new analysis that we use to create a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions.\n",
        "\n",
        "* __Bias__ measures how far off a models prediction is from the correct value for the response variable.\n",
        "\n",
        "* __Variance__ measures how much the prediction's vary between different versions of the model.\n",
        "\n",
        "__High bias__ is indicative of our model __underfitting__ the data, while __high variance__ is indicative of the model __overfitting__ our data.\n",
        "\n",
        "* To reduce high bias, we __increase the complexity of our model__, e.g, via the use of additional explanatory variables (when feasible).\n",
        "\n",
        "* To reduce high variance, we __reduce the complexity of our model__, e.g, by reducing the number of explanatory variables being used.\n",
        "\n",
        "Unfortunately, __decreasing model bias__ usually __increases model variance (and vice-versa)__, so data scientists need to understand how to strike a balance between the two.\n",
        "\n",
        "Our goal should always be to __strike a reasonable balance between bias and variance__ so that our model performs acceptably well without overfitting our training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4qWTuwABiC2"
      },
      "source": [
        "## Performance Metrics for Regression Models\n",
        "\n",
        "Sample output of a regression model constructed using the __statsmodels__ library:\n",
        "\n",
        "- __$R^2$__: Often referred to as \"Goodness of fit\"; measures how much of the variation in the response variable $y'$ is explained by variation in the explanatory variable(s). In general, __the larger the value of $R^2$, the more accurate the model is__. However, relatively large values of $R^2$ can also be an indication of the model being \"overfit\" to the training data.\n",
        "\n",
        "\n",
        "- __Adjusted $R^2$__: Increases if a new variable added to the model improves the fit of the model by more than would be expected by chance. When comparing two models derived from the same data, models with __higher__ __Adjusted__ $R^2$ scores are preferable to those having relatively lower scores.  Both $R^2$ and adjusted $R^2$ quantify regression. However, $R^2$ assumes that every single variable explains the variation in the dependent variable. The adjusted $R^2$ tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable (it has been adjusted for the number of predictors in the model). The adjusted $R^2$ will penalize a model for adding independent variables that do not fit the model (it can even be negative).\n",
        "\n",
        "\n",
        "- __AIC__: Akaike Information Criteria estimates the relative quality of a statistical model for a given set of data. It tries to select the model that most adequately describes an unknown, high dimensional reality.  When comparing two models derived from the same data, models with lower AIC scores are preferable to those having relatively higher scores.\n",
        "\n",
        "\n",
        "- __BIC__: Bayesian Information Criteria is another model selection metric that estimates the unexplained variation in the response variable relative to the given explanatory variables. BIC also imposes a \"complexity\" penalty when the number of explanatory variables used is increased.  BIC tries to find the TRUE model among the set of candidates.  When comparing two models derived from the same data, models with lower BIC scores are preferable to those having relatively higher scores. (See [here](https://www.methodology.psu.edu/resources/AIC-vs-BIC/).)\n",
        "\n",
        "\n",
        "- __F Statistic__: Indicates whether a significant amount of variance in the response variable $y'$ is explained by the model. When comparing two models derived from the same data, models with higher F Statistic scores are preferable to those having relatively lower scores. \n",
        "\n",
        "\n",
        "- __Log Likelihood__: A measure of how well a model fits the underlying data. When comparing two models derived from the same data, models with higher Log Likelihood scores are preferable to those having relatively lower scores. \n",
        "\n",
        "\n",
        "- __p values__: Measure the statistical significance of the explanatory variables in your model. The lower the p-value, the better the chance that we can reject the null hypothesis. For our purposes, the smaller the p-value, the better the chances that the feature has some relevance to our response variable and we should keep it.While you are free to select the significance level on your own, most often 0.05 is used as the maximum bound for significance. As such, if any variable in your model is shown to have a  p value that exceeds 0.05, consider removing it from the model to see whether the fit/model selection metrics improve.  For instance, a null hypothesis for a coin would be that it is a fair coin, if we were to flip it and get ten heads in a row, it would be \"very strange\", we would have a very low p-value, and reject the null hypothesis.  To assume data is important to us (and not just noise), we prefer it to be of the alternative hypothesis.\n",
        "\n",
        "\n",
        "- __Root Mean Squared Error (RMSE)__: Average distance of a sample from its observed value to its predicted value. We calculate the RMSE by finding the square root of the average of the squared values of a model's residual values. When comparing two models derived from the same data, models with lower RMSE  scores are preferable to those having relatively higher RMSE scores. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alNTiEjyBiC3"
      },
      "source": [
        "## Performance Metrics for Classifiers\n",
        "\n",
        "General confusion matrix + classification model performance metrics:\n",
        "\n",
        "__Confusion Matrix__\n",
        "\n",
        "1. True Positives (TP): True positives are the cases when the actual class of the data point was True and the predicted is also True\n",
        "Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.\n",
        "\n",
        "2. True Negatives (TN): True negatives are the cases when the actual class of the data point was False and the predicted is also False\n",
        "Ex: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.\n",
        "\n",
        "3. False Positives (FP): False positives are the cases when the actual class of the data point was False and the predicted is True. False is because the model has predicted incorrectly and positive because the class predicted was a positive one.\n",
        "Ex: A person NOT having cancer and the model classifying his case as cancer comes under False Positives.\n",
        "\n",
        "4. False Negatives (FN): False negatives are the cases when the actual class of the data point was True and the predicted is False. False is because the model has predicted incorrectly and negative because the class predicted was a negative one.\n",
        "\n",
        "See [here](https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b) for __Accuracy, Precision, Recall or Sensitivity, Specificity, F1_Score__.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "</br>\n",
        "\n",
        "__ROC (Receiver Operating Characteristic) Curve__: Calculate by plotting the true positive rate (TPR) against the false positive rate (FPR) [here](http://www.saedsayad.com/model_evaluation_c.htm).  Plotting TPR vs. FPR for a series of classification models constructed using different thresholds for predicting classifications yields a curve within a two-dimensional plane. The general algorithm when applied to a binary classifier works as follows:\n",
        "\n",
        "- Calculate the scored probabilities for the unknown classification values of every observation in your data set\n",
        "\n",
        "\n",
        "- Create an iterator + a set of data objects that can be used for capturing the output of the algorithm. You should have a distinct data object for each iteration you plan to run (e.g., a list or array or something similar of your own choosing). Each data object should contain 'n' elements where 'n' is equivalent to the number of observations in your model testing data set.\n",
        "\n",
        "\n",
        "- Use the iterator to iterate through a sequence of threshold values where (0 <= threshold <= 1) and compare the scored probability of each observation to the threshold. (In practice, the iterator will increase by .01 increments). If the scored probablitity for a given observation exceeds the threshold, assign a value of 1 to the appropriate item within the corresponding iteration-specific data object. Otherwise assign a value of 0.\n",
        "\n",
        "\n",
        "- Calculate and save the overall TPR and FPR values for the output of each iteration\n",
        "\n",
        "\n",
        "- Plot the resulting TPR and FPR values against one another in a two-dimensional plot.\n",
        "\n",
        "\n",
        "The resulting plot can be used to help you determine which threshold value is most effective for purposes of maximizing overall model performance.\n",
        "\n",
        "<br>\n",
        "</br>\n",
        "\n",
        "__Area Under the Curve (AUC)__: In a ROC plot, AUC is determined by calculating the area in the plot that falls __below / to the right of__ the ROC curve.  A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. The higher the AUC score, the better the performance of a model.  AUC scores from different models can be compared against one another to help us determine which model has the best performance.\n",
        "\n",
        "<br>\n",
        "</br>\n",
        "\n",
        "__Summary__\n",
        "\n",
        "We should always assess and compare model performance using a variety of metrics. In some instances, certain metrics may be much more relevant than others. It is up to you as a data practitioner to decide which model performance metrics are most meaningful relative to both the data you are working with + the questions you are trying to answer with that data + the type(s) of models you have elected to employ. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJIi7x1lUhHH"
      },
      "source": [
        "#Ensemble Learning\n",
        "\n",
        "This refers to the practice of aggregating the output of multiple models in an attempt to improve the quality of your predictions above and beyond what is possible using a single high-quality model.\n",
        "\n",
        "A group of predictive models is referred to as an “ensemble”, and an ensemble learning algorithm is referred to as an “ensemble method”. A well-known example of an ensemble model is a random forest: random forests are comprised of multiple individual decision trees, each of which contributes its output to an aggregated set of predictions which is then used to produce the predictive outputs of the random forest model.\n",
        "\n",
        "Data scientists can also create their own combinations of predictive models when trying to attain an improved overall quality of model performance. For example, when addressing a numerical regression task, practitioners might choose to leverage the output of multiple distinct relatively high performing regression models derived from a single data set rather than rely solely on the output of a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aanUbMvqUiUW"
      },
      "source": [
        "# __Day_4 Assignment__ \n",
        "\n",
        "### Understanding Classification Model Performance Metrics \n",
        "\n",
        "Evaluation of the performance of classification models can be facilitated through a combination of calculating certain types of performance metrics and generating model performance evaluation graphics. For this assignment you are tasked with calculating a suite of classification model performance metrics via Python code functions that you  create and then comparing your results to those of pre-built Python functions that automatically calculate  those same metrics. You will also be creating graphical output via Python code that can be used to evaluate  the performance of classification models.  \n",
        "\n",
        "The data set you will be using for this work is comprised of approximately 180 labeled binary observations  (i.e., the classification is binary in nature). The classifications are either ‘0’ or ‘1’. A classification model has  been used to “predict” the actual classifications for each observation, and its algorithm works by estimating  the probability that the correct classification for each observation is a ‘1’. The data set includes three  attributes that you will make use of for your analysis: \n",
        "\n",
        "1. class: the actual classification for the observation \n",
        "2. scored.class: the predicted classification for the observation (can only be ‘0’ or ‘1’; identified by  comparing the classification model’s probability score (i.e., the likelihood that the correct classification  for an observation is a ‘1’) for the observation against a 0.50 threshold)) \n",
        "3. scored.probability: the classification model’s probability score (i.e., the likelihood that the correct  classification for an observation is a ‘1’) for the observation \n",
        "\n",
        "<br>\n",
        "</br>\n",
        "\n",
        "### Problem \n",
        "1. Load the data from github into a Pandas df\n",
        "\n",
        "2. Use Pandas’ crosstab() function to calculate the contents of a confusion matrix for the data. Make sure  you closely examine the output e.g., do the rows represent the actual or the predicted classification?  What about the columns? (HINT: A good way to ensure you understand the output of the crosstab() function is to check the value_counts() for the dataframe columns you’ve used as input to the  crosstab() function. When observing the value_counts(), determine whether the rows or columns in  the crosstab() output sum to the respective value_counts() figures).\n",
        "\n",
        "3. Extract the individual confusion matrix values (i.e., True Positive, False Positive, True Negative, False  Negative) from the output of the crosstab() function and save them for later use (e.g., save them each  to individual variables or to the data structure of your choice). Knowing how to properly extract these  values from the output of crosstab() will serve you well throughout the remainder of the assignment \n",
        "\n",
        "4. Write a Python function that accepts as input the actual and predicted classifications for any binary  classification data and then calculates and returns the accuracy metric for the predictions without  utilizing any pre-built Python accuracy metric calculation functions.  Do the same for precision, sensitivity, specificity, and F1_score.\n",
        "\n",
        "5. Write a Python function to plot a ROC curve and also calculate AUC for any binary classification data  that contains both actual and predicted classifications as well as the associated scored probabilities without utilizing any pre-built Python classification metric calculation or ROC or AUC functions. \n",
        "\n",
        "6. Apply the Python functions you’ve created to the relevant columns of the provided data set to produce  the classification metrics specified.\n",
        "\n",
        "7. Compare the output of your functions against the output of the pre-built functions available  within the scikit-learn library. Specifically, apply the confusion_matrix(), accuracy_score(),  precision_score(), recall_score() (remember: recall = sensitivity), f1_score(), and the  metrics.classification_report() functions to the relevant columns of the provided data set.\n",
        "\n",
        "8. Using the metrics.plot_roc_curve () and metrics.auc() functions from the scikit-learn package to  generate a ROC plot and calculate AUC for the provided data set. How do the results compare with the  ROC/AUC function you’ve created for this assignment? \n",
        "\n"
      ]
    }
  ]
}
