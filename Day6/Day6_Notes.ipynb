{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Day6_Notes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewFried/Udemy/blob/master/Day6/Day6_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V94s3jrfEywk"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#Day 6: Regression Modeling for Categorical  Response Variables\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TA2oqkBEywn"
      },
      "source": [
        "## Regression Modeling for Categorical Response Variables\n",
        "\n",
        "- __Binary Logistic Regression__: The response variable is a __binary categorical variable__, while the explanatory variables can be either continuous, discrete or binary (e.g., dummy variables created from the values of a categorical variable).\n",
        "\n",
        "\n",
        "- __Multinomial Logistic Regression (MLR)__: The response variable is a categorical variable having __more than two (2) possible values__, while the explanatory variables can be either continuous, discrete or binary. There are many types of MLR models, and the exact type of MLR model you choose to use will be dependent on whether your response variable is __ordinal__ or __nominal__ in nature.\n",
        "\n",
        "\n",
        "### Binary Logistic Regression\n",
        "\n",
        "In binary logistic regression, the response variable is __binary__, i.e., it contains data encoded as either '1' (e.g., 'True') or '0' (e.g., 'False'). In other words, logistic regression is used for __classification__ problems.\n",
        "\n",
        "Logistic regression finds the best fitting __mathematical model__ (not a simple linear equation) to describe the relationship between the binary response variable and the explanatory variable(s). The values it generates are the coefficients of a formula to predict a __logit transformation__ (aka \"__log odds\"__) which is the logarithm of the odds $p$ of the probability of the presence of the characteristic of interest. \n",
        "\n",
        "$logit(p) = ln(p/(1-p)) = b0 + b1x1 + b2x2 + .. + bnxn$\n",
        "\n",
        "where $p$ = the probability of the __presence__ of a characteristic,\n",
        "\n",
        "$(1-p)$ = the probability of the __abscence__ of a characteristic,\n",
        "\n",
        "$b0$ = is a constant value\n",
        "\n",
        "$b1, b2, ..,bn$ = the regression coefficients for the explanatory variables $x1, x2, .., xn$\n",
        "\n",
        "Effective binary logistic regression models contain little to no collinearity amongst the explanatory variables used.\n",
        "\n",
        "The explanatory variables should be linearly related to the log odds of the probability of the presence of the characteristic of interest (i.e., the value of the response variable)\n",
        "\n",
        "Logistic regression generally does not work well when applied to very small data sets. For observational studies, a minimum of __500 samples/observations__ is recommended.\n",
        "\n",
        "Further explanation is available here:\n",
        "\n",
        "https://www.statisticssolutions.com/what-is-logistic-regression/\n",
        "\n",
        "\n",
        "\n",
        "### Multinomial Logistic Regression\n",
        "\n",
        "In multinomial logistic regression (MLR), the response variable is __multiclass__, i.e., it has __more than 2 possible values__. Multinomial regression calculates the probability of any observation being assigned to each of the possible classes. Therefore, the regression model must perform at least $n-1$ calculations, where $n$ is the number of possible classifications for the response variable.\n",
        "\n",
        "Why $n-1$ calculations? If we have $n$ classes, we can determine the probability of the $n$th class by simply summing the probabilities for each of the other classifications and subtracting that sum from $1$, i.e., \n",
        "\n",
        "### $P(Class_n) = 1 - ( P(Class_1) + P(Class_2) + ... + P(Class_{n-1}) ) $\n",
        "\n",
        "As with binary logistic regression, the coefficients of a MLR model predict the __log odds__ of the probability of any observation belonging to each of the $n$ classes. \n",
        "\n",
        "The probability of observing class $k$ out of $n$ total classes is:\n",
        "\n",
        "# $ P(y_i = k) = \\frac { e^{S_k} }  {e^{S_1} + e^{S_1} + ... + e^{S_1} } $\n",
        "\n",
        "(see https://towardsdatascience.com/understanding-logistic-regression-coefficients-7a719ebebd35)\n",
        "\n",
        "When performing multiple logistic regression, the algorithm assigns a predicted classification for a given observation based on __which of the possible classifications achieves the largest log odds value__.\n",
        "\n",
        "\n",
        "### How to interpret logistic regression model coefficients\n",
        "\n",
        "\"A one unit increase in explanatory variable $x$ (with all other explanatory variables held constant) increases the log-odds of response variable $y$ by the amount indicated by the coefficient of $x$.\"\n",
        "\n",
        "However, in practice it is very common to simply examine the __directionality__ of the coefficients, i.e., if a coefficient is __negative__, then the larger the value of the explanatory variable, the more it will tend to __decrease__ the magnitude of the response variable. Conversely, if a coefficient is __positive__, then the larger the value of the explanatory variable, the more it will tend to __increase__ the magnitude of the response variable.\n",
        "\n",
        "\n",
        "### How to convert log odds to a probability\n",
        "\n",
        "- __Step 1__: Convert the log odds value to odds via __exponentiation__, i.e., the antilog of a log value. Assuming the log odds value we've been given is a __natural log__, we apply an __exponential function__ to convert the log odds value to and odds value.\n",
        "\n",
        "\n",
        "- __Step 2__: Calculate the probability as follows: \n",
        "\n",
        "### $ Prob = \\frac {odds} {1 + odds} $\n",
        "\n",
        "http://www.pmean.com/13/predicted.html\n",
        "\n",
        "\n",
        "### Advantages of Logistic Regression\n",
        "\n",
        "- Relatively computationally efficient + easy to implement\n",
        "\n",
        "\n",
        "- Relatively easy to interpret (though not as easy to interpret as linear regression models)\n",
        "\n",
        "\n",
        "- Can be effective even if features have not been scaled to similar ranges\n",
        "\n",
        "\n",
        "- Due to its simplicity, logistic regression is widely used as a baseline when comparing the performance of other more complex classification methods (e.g., decision trees, random forest, KNN, neural networks, etc.)\n",
        "\n",
        "\n",
        "### Disadvantages of Logistic Regression\n",
        "\n",
        "- Due to its simplicity, performance can lag behind that of more complex classification methods\n",
        "\n",
        "\n",
        "- Requires a __linear__ relationship between the explanatory variables and the response variable since it relies on a linear decision space\n",
        "\n",
        "\n",
        "- Requires that explanatory variables are __very__ independent of one another + that they have a substantive relationship with the response variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HgsYpm0Eywn"
      },
      "source": [
        "### Evaluating Logistic Regression Model Performance\n",
        "\n",
        "We learned about a wide variety of classification model performance metrics, all of which can be used for purposes of evaluating the performance of any type of logistic regression model:\n",
        "\n",
        "- Confusion Matrices: True Positives (TP), False Positives (FP), True Negatives (TN), False Negatives\n",
        " (FN)\n",
        " \n",
        " \n",
        "- Accuracy\n",
        "\n",
        "\n",
        "- Precision \n",
        "\n",
        "\n",
        "- Recall / Sensitivity\n",
        "\n",
        "\n",
        "- Specificity\n",
        "\n",
        "\n",
        "- F1 Score\n",
        "\n",
        "\n",
        "- ROC: __ROC (Receiver Operating Characteristic) Curve__: Calculate by plotting the __true positive rate (TPR)__ against the __false positive rate (FPR)__; http://www.saedsayad.com/model_evaluation_c.htm. Plotting TPR vs. FPR for a series of classification models constructed using different thresholds for predicting classifications yields a curve within a two-dimensional plane.\n",
        "\n",
        "\n",
        "- AUC: In a ROC plot, AUC is determined by calculating the area in the plot that falls __below / to the right of__ the ROC curve.  A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. The higher the AUC score, the better the performance of a model.  AUC scores from different models can be compared against one another to help us determine which model has the best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d610TXJeEywo"
      },
      "source": [
        "## Logistic Regression example with scikit-learn\n",
        "\n",
        "Here is the 'Titanic' data set which contains passenger survival rate data for the ocean-going vessel of that name that sank in 1912. \n",
        "\n",
        "Our goal is to __try to predict whether or not a passenger would have been more likely than not to survive the sinking of the Titanic__ based on the data contained in the data set (i.e., build a __predictive model__).\n",
        "\n",
        "- First, we will fit a logistic regression model on a __training__ data set using the data set's __PClass__, __Age__, and __Sex__ attributes.\n",
        "\n",
        "\n",
        "- Then we evaluate the model using a __testing__ data set that is exclusive of the data contained in the __training__ data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOh0YeCOEywp",
        "outputId": "2f8e7efd-6d88-4ba3-98b7-10c5d9efe1e2"
      },
      "source": [
        "# load the LogisticRegression() function from sklearn's 'linear_model' sub-library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# load the training + testing data sets for the Titanic data\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/test.csv')\n",
        "\n",
        "# display the first four rows of the data set\n",
        "train[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve4lQlzmEywu",
        "outputId": "45df5dc0-b68c-429e-af90-59d33360e337"
      },
      "source": [
        "# lets check the first four rows of the test data\n",
        "# Note the lack of a \"Survived\" indicator !!!\n",
        "test[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>IsFemale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass                              Name     Sex   Age  SibSp  \\\n",
              "0          892       3                  Kelly, Mr. James    male  34.5      0   \n",
              "1          893       3  Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n",
              "2          894       2         Myles, Mr. Thomas Francis    male  62.0      0   \n",
              "3          895       3                  Wirz, Mr. Albert    male  27.0      0   \n",
              "\n",
              "   Parch  Ticket    Fare Cabin Embarked  IsFemale  \n",
              "0      0  330911  7.8292   NaN        Q         0  \n",
              "1      0  363272  7.0000   NaN        S         1  \n",
              "2      0  240276  9.6875   NaN        Q         0  \n",
              "3      0  315154  8.6625   NaN        S         0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ptEx6VIEywz",
        "outputId": "1bd139b4-9df1-4e42-b398-1bbd7e2c815e"
      },
      "source": [
        "# how many records in the training data set?\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(891, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWrl1zCYEyw3",
        "outputId": "70612024-5163-4651-d308-925a4cf26320"
      },
      "source": [
        "# how many people survived in the training set?\n",
        "train.Survived.values.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY1uXT2VEyw6",
        "outputId": "4c3d8fc0-fcee-4e48-cddb-eb94c2083dfa"
      },
      "source": [
        "# what percentage of the training set survived?\n",
        "train.Survived.values.sum() / train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3838383838383838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKBu_n_SEyw-"
      },
      "source": [
        "__NOTE__: Since we know that 38.3% of the people in the training set survived, we could achieve a training model accuracy of (1 - .383) = 61.7% by simply predicting \"Did not survive\" for each passenger. This metric is referred to as the __null error rate__.  When evaluating the performance of a binary logistic regression model, always check to see whether the accuracy you are attaining exceeds the __null error rate__. If not, your model is unlikely to be of any value.\n",
        "\n",
        "\n",
        "Missing data will generally prevent the fitting of a model via either __scikit-learn__ or __statsmodels__, so we must first check both the training and testing data for missing data values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRujBMaVEyxA",
        "outputId": "925124e7-8f5c-466a-90c7-566314f66cc7"
      },
      "source": [
        "# check the training data for null values\n",
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaWaOYJJEyxF"
      },
      "source": [
        "Within the training data, the __Age__, __Cabin__ and __Embarked__ attributes all have missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2UYC_MEyxG",
        "outputId": "09805f81-72dd-4d07-bf44-b1958a5ec47b"
      },
      "source": [
        "# check the test data for null values\n",
        "test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age             86\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             1\n",
              "Cabin          327\n",
              "Embarked         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAgtiQ15EyxJ"
      },
      "source": [
        "Within the testing data, the __Age__, __Fare__ and __Cabin__  attributes all have missing data.\n",
        "\n",
        "So if we want to use __Age__ as part of our predictive model, we need to somehow fill the missing __Age__ values. In this instance, the author of the PfDA text chooses to use the relatively simple process of filling the missing values with the __median__ Age value found within the __training__ data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j_FopU6EyxJ"
      },
      "source": [
        "# find the median 'Age' value within the training data set\n",
        "impute_value = train['Age'].median()\n",
        "\n",
        "# now fill the missing 'Age' values in both the training and testing data sets\n",
        "train['Age'] = train['Age'].fillna(impute_value)\n",
        "test['Age'] = test['Age'].fillna(impute_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a17xCnjHEyxN"
      },
      "source": [
        "Next, __we create a dummy indicator for the 'Sex' categorical variable__: the new dummy variable 'IsFemale' contains a '1' if the 'Sex' value for a passenger is 'Female' and a '0' otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_mqr8QdEyxN"
      },
      "source": [
        "# create a dummy variable for the 'Sex' attribute in both the training and\n",
        "# testing data sets\n",
        "train['IsFemale'] = (train['Sex'] == 'female').astype(int)\n",
        "test['IsFemale'] = (test['Sex'] == 'female').astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpujAKUEyxR"
      },
      "source": [
        "Now we are ready to define our predictive model using the attributes we want to use as explanatory variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCu9NvheEyxR",
        "outputId": "4ab7ac4b-a45f-46a5-b4c1-625d629730e7"
      },
      "source": [
        "# define a vector containing the names of the attributes to use\n",
        "predictors = ['Pclass', 'IsFemale', 'Age']\n",
        "\n",
        "# create a subset of the training data using ONLY the selected explanatory variables\n",
        "X_train = train[predictors].values\n",
        "\n",
        "# create a subset of the testing data using ONLY the selected explanatory variables\n",
        "X_test = test[predictors].values\n",
        "\n",
        "# isolate the response indicator for the training data\n",
        "y_train = train['Survived'].values\n",
        "\n",
        "# sanity check on training data\n",
        "X_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.,  0., 22.],\n",
              "       [ 1.,  1., 38.],\n",
              "       [ 3.,  1., 26.],\n",
              "       [ 1.,  1., 35.],\n",
              "       [ 3.,  0., 35.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLxTCkw9EyxW",
        "outputId": "e749b8fe-b4bc-49d2-9ec2-a5f2a89ee3b3"
      },
      "source": [
        "# sanity check on response indicator\n",
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 0], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNVbZnqPEyxZ",
        "outputId": "e35562a9-ebb0-428a-ebea-b39700eeafa3"
      },
      "source": [
        "# We're using the LogisticRegression() method for this model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# fit the model: X_train contains our explanatory variables while \n",
        "# y_train contains the response variable\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\James T\\Anaconda3\\envs\\DAV5400\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UFUjj5QEyxd",
        "outputId": "e780bb4f-8c7a-468d-d2c0-199b86c57e63"
      },
      "source": [
        "# calculate the accuracy of the model relative to the training data set\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7946127946127947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1wVWzQIEyxg"
      },
      "source": [
        "Recall from above we calculated the __null error rate__ for our data to be 61.7%. The model we've generated has an accuracy score of 79.46%. As such, our model appears to be useful.\n",
        "\n",
        "__What can we learn by examining the regression model coefficients for the explanatory variables?__\n",
        "\n",
        "While the actual numeric values of the coefficients of a regression model are often very difficult to interpret, we can use the fact that a coefficient is either positive or negative to determine what effect of an __increase__ in the value of a given variable will have on the predicted value of the response variable. \n",
        "\n",
        "In this binary logistic regression example, the value of the response variable is the \"log odds\" of the binary outcome being either a '0' or '1' ('0' meaning the passenger was more likely to perish while '1' indicates the passenger was more likely to survive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjEPzI5mEyxh",
        "outputId": "1da8d3cc-1f95-44f3-aed2-d885e3f8740d"
      },
      "source": [
        "# examine the model coefficients for the explanatory variables\n",
        "print(predictors)\n",
        "model.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Pclass', 'IsFemale', 'Age']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.07373582,  2.52093733, -0.02864864]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0dMFrYzEyxk"
      },
      "source": [
        "From the above we see that:\n",
        "\n",
        "- __Passenger Class__: An increase in the value of 'Passenger_Class' is associated with a __decreased__ likelihood of survival, i.e., passengers in lower classes of service were more likely to perish than were passengers in relatively higher classes of service.\n",
        "\n",
        "\n",
        "- __IsFemale__: Being female increased the likelihood of survival\n",
        "\n",
        "\n",
        "- __Age__: The older the passenger, the less likely they were to survive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UBPN6EbEyxl",
        "outputId": "23ef8d88-933c-4b82-80e6-631e9c22bb64"
      },
      "source": [
        "# generate predictions for the test data using our new model\n",
        "y_predict = model.predict(X_test)\n",
        "y_predict[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouXW4ZJqEyxn"
      },
      "source": [
        "The 'y_predict' array contains predictions that answer the question: \"__Was a given passenger more likely than not to have survived the sinking of the Titanic?__\"\n",
        "\n",
        "If we had the actual __survived/perished__ indicators for the test data, we could now check the performance of the model via a variety of error metrics (e.g., accuracy, specificity, precision, recall, AUC, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MScAAknnEyxo"
      },
      "source": [
        "### Case Study: Logistic Regression with scikit-learn\n",
        "\n",
        "[Link](https://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oXlZixhEyxp"
      },
      "source": [
        "# **Day6 Assignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBUdNiMXnXzC"
      },
      "source": [
        "##Logistic Regression \r\n",
        "\r\n",
        "Banks are highly regulated. They must adhere to strict lending guidelines on several fronts. The must avoid discrimination when  approving or rejecting loan applications. \r\n",
        "\r\n",
        "Many years ago, banks engaged in the practice of “redlining” when approving  loans for home mortgages.  They would reject loan applications for homes located  in certain areas that were deemed by banks to be overly prone to loan defaults. When such practices were deemed to  be discriminatory by bank regulators, banks adjusted their mortgage lending practices in an attempt to make them more  objective. However, regulators remain concerned that discrimination can exist even when what appear to be purely  objective criteria are used for purposes of deciding whether or not a customer is approved for a loan. \r\n",
        "\r\n",
        "Develop a model that can predict whether or not a given mortgage  loan application is likely to be approved or denied.\r\n",
        "\r\n",
        "The data set you will be using is sourced from the Federal Reserve Bank of Boston. [Here](https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Hdma.html) is a brief overview of the data.\r\n",
        "\r\n",
        "It has 2,381 observations of 1 response/dependent variable (which indicates whether or  not a mortgage application was denied) and 12 explanatory/independent variables. \r\n",
        "\r\n",
        "Construct and compare/contrast a series of binary logistic regression  models (after completing the necessary EDA and data prep work) that predict whether or not a given  mortgage application is likely to be denied. The response variable is the data set’s  “DENY” attribute, which indicates whether or not a mortgage application was denied. \r\n",
        "\r\n",
        "###Process\r\n",
        "\r\n",
        "1. Load the [data](https://raw.githubusercontent.com/MatthewFried/Udemy/master/Day6/Day6%20Data%20-%20Federal%20Reserve.csv) into a pandas DataFrame and perform the EDA. Do the data preparation work. Make  sure to Describe and show the steps you have taken to address the data integrity and usability issues you identified in your EDA, including any feature engineering techniques you have  applied to the data set. \r\n",
        "\r\n",
        "2. Do feature selection and/or dimensionality reduction techniques to identify  explanatory variables for inclusion within your models. You should apply domain knowledge, use forward or backward selection, or use a different feature  selection method (e.g., decision trees, etc.). \r\n",
        "\r\n",
        "3. After splitting the data into training and testing subsets, use the training subset to construct at least  three different binomial logistic regression models using different combinations of explanatory  variables (or the same variables if they have been transformed via different transformation methods). \r\n",
        "\r\n",
        "4. Decide how you will select the “best” regression model from those  you have constructed. For example, are you willing to select a model with slightly lower performance if  it is easier to interpret or less complicated to implement? What metrics will you use to  compare/contrast your models? Evaluate the performance of your models via cross validation using  the training data set. Then apply your preferred model to the testing subset and assess how well it  performs on that previously unseen data. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXxAZRLzI53"
      },
      "source": [
        "##**Review of Assignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhKimo-PyuWV"
      },
      "source": [
        "The __deny__ responsible variable is an __imbalanced class__: more than 88% of its values are '0' while less than 12% are '1'. The __null error rate__ for the response variable is 0.882, which means that we could achieve 88.2% accuracy by simply predicting that a loan is not denied for every observation within the data set. Obviously, such a model wouldn't be very useful for purposes of deciding whether or not a mortgage application should be denied.\r\n",
        "\r\n",
        "We learned that reliance on __accuracy__ for purposes of comparing similar models __is not appropriate when the response variable is an imbalanced class__. \r\n",
        "\r\n",
        "Which metrics should we rely on?\r\n",
        "\r\n",
        "- __precision__: TP / (TP + FP) If we are trying to __minimize the number of false positives (FP)__, we should use __precision__ as one of our primary model performance metrics.\r\n",
        "\r\n",
        "\r\n",
        "- __recall__: TP / (TP + FN) If we are trying to __maximize the number of true positives (TP)__ or __minimize the number of false negatives__, we should use __recall__ as one of our primary model performance metrics. Why? Maximization of TP necessarily minimizes instances of FP.\r\n",
        "\r\n",
        "\r\n",
        "- __F1 Score__: 2 * (precision * recall) / (precision + recall) is the __weighted average of precision and recall__. Therefore, F1 score __is a measure of how well a model handles both false positives and false negatives__. Remember: models with relatively larger F1 scores are preferable to models having relatively smaller F1 scores.\r\n",
        "\r\n",
        "\r\n",
        "#### How else might we handle imbalanced classes?\r\n",
        "\r\n",
        "- Synthetic Minority Oversampling Technique (\"SMOTE\"): The concept of SMOTE was explained in the Module 4 Assigned readings (see __Machine Learning Pocket References, Chapter 9__). SMOTE works by __synthesizing__ new examples from the minority class. The process works as follows:\r\n",
        "\r\n",
        "- A random example from the minority class is chosen. \r\n",
        "\r\n",
        "\r\n",
        "- k of the nearest neighbors for that example are found (typically k=5). \r\n",
        "\r\n",
        "\r\n",
        "- An additional randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\r\n",
        "\r\n",
        "\r\n",
        "This process is repeated as many times as needed to balance out the classifications for the imbalanced variable\r\n",
        "\r\n",
        "The approach is effective because new synthetic examples from the minority class are created that are __plausible__, meaning their features are relatively to those of existing examples from the minority class.\r\n",
        "\r\n",
        "When finished, we then have a balanced class. So if our variable is a binary categorical feature, the __null error rate__ for the data that includes the synthesized observations will be __.50__. With a .50 null error rate, an accuracy metric can be a very effective tool for comparing models.\r\n",
        "\r\n",
        "For more details + examples see this [link](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\r\n",
        "\r\n",
        "#### So should we always use SMOTE if we have an imbalanced response variable?\r\n",
        "\r\n",
        "Not necessarily. It can be effective in some instances but is not guaranteed to improve model performance. As always, we need to test it empirically to judge its efficacy relative to a specific data set.\r\n"
      ]
    }
  ]
}