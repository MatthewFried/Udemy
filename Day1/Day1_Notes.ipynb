{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Day1_Notes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewFried/Udemy/blob/master/Day1_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz5v_49cvRe3",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# __Module 1: Getting Started__\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg0FCszyvRe6",
        "colab_type": "text"
      },
      "source": [
        "## What is the Life Cycle of a Typical Data Science Project?\n",
        "\n",
        "__Step 1__: Define a question\n",
        "\n",
        "__Step 2__: Identify and acquire the data\n",
        "\n",
        "__Step 3__: Exploratory Data Analysis (EDA): including the derivation + interpretation of relevant summary statistics, formulation of appropriate exploratory graphics, including, but not limited to, histograms, bar plots, box plots, correlation matrices, quartiles, IQR's, means, medians, standard deviations, variances, etc.\n",
        "\n",
        "__Step 4__: Data Preparation (e.g, clean the data; transform the data into an appropriate format; create visualizations; etc.)\n",
        "\n",
        "__Step 5__: Data Splitting (e.g., separating data into training, validation, testing subsets)\n",
        "\n",
        "__Step 6__: Model Training and Selection\n",
        "\n",
        "__Step 7__: Model Testing\n",
        "\n",
        "__Step 8__: Communicate your findings and revise model and/or data as needed (repeat Steps 2 - 7 as necessary)\n",
        "\n",
        "__Step 9__: Model Deployment\n",
        "\n",
        "<br>\n",
        "\n",
        "A lot of the time spent in modeling is on steps 1-4. Since the proper use of machine learning algorithms is usually dependent upon the type of data to be analyzed, be *extremely* careful when choosing your data.\n",
        "\n",
        "<br>\n",
        "\n",
        "We will be using/doing the following:\n",
        "  * Anaconda\n",
        "  * Jupyter or Colab Notebooks \n",
        "  * Python programming, including how to use + implement user-defined functions, comments, MatPlotLib, Seaborn, etc.\n",
        "  * Creating a narrative within our analysis\n",
        "  * Uploading data and outputs to Github  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUrAA1brvRe9",
        "colab_type": "text"
      },
      "source": [
        "## __Reference Texts__\n",
        "\n",
        "-\t_Feature Engineering and Selection_ (Kuhn, Johnson), CRC Press\n",
        "\n",
        "\n",
        "-\t_Machine Learning Pocket Reference_ (Matt Harrison), O’Reilly\n",
        "https://github.com/mattharrison/ml_pocket_reference (link to all Python code examples provided in textbook)\n",
        "\n",
        "\n",
        "-\t_Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow, 2nd Edition_ (Aurelien Geron) O’Reilly\n",
        "https://github.com/ageron/handson-ml2 (link to sample Python code examples provided in textbook)\n",
        "\n",
        "\n",
        "-\t_Data Science From Scratch, 2nd Edition_ (Joel Grus), O’Reilly\n",
        "https://github.com/joelgrus/data-science-from-scratch (link to sample Python code examples provided in textbook)\n",
        "\n",
        "\n",
        "-\t_Data Science for Business_ (Provost, Fawcett) O’Reilly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zpfOD6SvRe_",
        "colab_type": "text"
      },
      "source": [
        "## __Keywords__\n",
        "\n",
        "- An __\"attribute\"__ (a.k.a., __\"feature\"__) is a data field or variable that represents a single aspect of an observation. For example, if we are recording meteorological data via a weather station, we would be collecting attributes such as temperature, wind speed, humidity, and ambient air pressure. Each column represents a distinct variable, refered to as an __\"attributes\"__ or __\"features\"__.\n",
        "\n",
        "\n",
        "- An __\"observation\"__ represents a collections of attributes that comprise the characteristics of a single data record. In other words, it is the row. (__NOTE__: Some scientists also refer to such collections of attributes as __\"feature vectors\"__ or __\"attribute vectors\"__).\n",
        "\n",
        "\n",
        "- A __\"response\" (aka \"Dependent\")__ variable describes the output attribute of a model. For example, when constructing a predictive model, the variable we are attempting to predict would identified as the __\"response\"__ variable.\n",
        "\n",
        "\n",
        "- By contrast, __\"Explanatory\" (aka \"Independent\" or \"Predictor\")__ are the input variables we will estimate.\n",
        "\n",
        "<br>\n",
        "\n",
        "## __Types of Machine Learning Systems__\n",
        "\n",
        "- __Supervised Learning__: The data set used to train the machine learning model __includes__ \"labels\" for the response variables fore each observation. Examples include regression models, support vector machines, decision trees, Naive Bayes models, K-nearest neighbor models, and many neural network algorithms.\n",
        "\n",
        "\n",
        "- __Unsupervised Learning__: The data for the model contains no data values for the desired response variable. Examples include clustering algorithms and some neural network algorithms. \n",
        "\n",
        "\n",
        "- __Semisupervised Learning__: The data set used to train the machine learning model is __partially labeled__, i.e., some observations contain the data value for the desired response variable while some do not. Most semisupervised machine learning algorithms are constructed by combining supervised and unsupervised algorithms. Examples include deep belief networks (DBN's) and restricted Boltzman machines (RBM's).\n",
        "\n",
        "\n",
        "- __Reinforcement Learning__: Reinforcement learning is much more computationally complex than supervised or unsupervised learning. It requires the implementation of an \"agent\" that can observe its environment and then initiate actions in response to what it has observed. Examples of reinforcement learning include some robotics applications that incorporate sensors and/or machine vision components. \n",
        "\n",
        "<br>\n",
        "\n",
        "### __Online vs. Batch Machine Learning Systems__\n",
        "\n",
        "- __Online__ machine learning systems learn incrementally from a continuous stream of incoming data. Capital market feeds would be an example of this. \n",
        "\n",
        "\n",
        "- By contrast, __batch__ machine learning systems require the inputting of all available data for purposes of model training.\n",
        "\n",
        "<br>\n",
        "\n",
        "### __Instance Based vs. Model Based Machine Learning Systems__\n",
        "\n",
        "- __Instance Based__ machine learning systems rely on __similarity metrics__ for purposes of generating predictions for previously unseen data observations. Examples of __instance based__ machine learning algorithms include K-nearest neighbors and K-means clustering.\n",
        "\n",
        "\n",
        "- By contrast, __model based__ machine learning algorithms require that we embed all of our assumptions about the problem we are trying to solve within the form of a model and then use that model to make predictions for the response variable of a previously unseen data observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwiKxffwvRfA",
        "colab_type": "text"
      },
      "source": [
        "## Main Challenges of Machine Learning\n",
        "\n",
        "- __Insufficient Data__: Models with insufficient data can be highly variable and inaccurate\n",
        "\n",
        "- __Nonrepresentative Training Data__: If the data we train a model is not representative of the actual data it will be highly ineffective.\n",
        "\n",
        "\n",
        "- __Sampling Bias__: Training a model with non-random samples of a population is sampling bias. It underrepresents certain sub-populations of the broader population. \n",
        "\n",
        "- __Sampling Noise__: If our model is trained on a data set that is too small to accurately reflect the broader population it will be ineffective when applied to previously unseen data from that broader population. \n",
        "\n",
        "- __Poor Quality Data__: If our training data is representative but suffers from too many missing or invalid data values, the resulting model is likely to be ineffective.\n",
        "\n",
        "- __Irrelevant Features__: Our model will clearly be ineffectual with irrelevant features.\n",
        "\n",
        "- __Overfitting__: Our model is highly accurate when applied to our training data but relatively inaccurate when applied to previously unseen data. This is typical when using a polynomial regression.\n",
        "\n",
        "\n",
        "- __Underfitting__: Our model is too \"simple\" relative to the broader population of data we intend to apply it to and as a result its output is not very useful. This is typical when using a linear regression instead of a more appropriate regression such as an exponential regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU5FOyme19Kj",
        "colab_type": "text"
      },
      "source": [
        "# __Model Complexity__\n",
        "###__Bias Vs Variance__\n",
        "\n",
        "![Data](https://drive.google.com/uc?export=view&id=1SaZvz1Z2bKl-AcP2LjghM5XDctckgZJ3)\n",
        "\n",
        "* Bias is the difference between the Predicted Value and the Expected Value. \n",
        "* Variance is when the model takes into account the fluctuations in the data, including the noise. When there is high variance, the model learns too much from the training data, it is called overfitting. \n",
        "<br>\n",
        "\n",
        ">- A model with a high bias error underfits data and makes very simplistic assumptions on it\n",
        ">- A model with a high variance error overfits the data and learns too much from it\n",
        ">- A good model is where both Bias and Variance errors are balanced\n",
        "\n",
        "![Data](https://drive.google.com/uc?export=view&id=1raQvJJcpMqz-9afJMqoG45DeS7EBLEz6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boPMlm0W610f",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# __Training Data__\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzA5Q65U9i5U",
        "colab_type": "text"
      },
      "source": [
        "### Data Splitting: Training, Evaluation / Validation, and Testing Subsets\n",
        "\n",
        "- Machine learning models should be trained and tested on distinct subsets of the available data.\n",
        "\n",
        "- Set aside 20%-35% of the available data for model testing.\n",
        "\n",
        "\n",
        "- There is no \"ideal\" testing/training split. In general, the larger the amount of data, the more you can safely set aside for model testing while reducing the chance of an ineffective model being produced.\n",
        "\n",
        "\n",
        "- Use __random sampling__ to create the training and testing subsets unless you are working with temporal data (e.g., time series data).\n",
        "\n",
        "\n",
        "- A portion of the training subset (e.g, a random sample of 5% - 10% of the training subset) should then be set aside for purposes of evaluating / validating the results of the model training process (hence the name __Evaluation__ or __Validation__ subset).  There are several approaches to  this.\n",
        "\n",
        "- Instead of using a data subset for distinct evaluation / validation we can use __cross validation__ instead.\n",
        "\n",
        "<br>\n",
        "\n",
        "### __Automated Data Splitting via scikit-learn: An Example__\n",
        "\n",
        "scikit-learn's __train_test_split()__ function can be used to create training and testing subsets. However, we must first split our response variable from our other variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uChB9gS318CB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "4487fa6a-0c7b-4367-8efb-dcf10076e533"
      },
      "source": [
        "# load the pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# load the train_test_split function from the sklearn.model_selection module\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# start by reading a set of sample data from github. This data contains data related to wines\n",
        "filename = \"https://raw.githubusercontent.com/MatthewFried/DAV6150/master/M3_Data.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df.head()\n",
        "\n",
        "#set up a clean copy as a safety for later\n",
        "df_copy = df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INDEX</th>\n",
              "      <th>TARGET</th>\n",
              "      <th>FixedAcidity</th>\n",
              "      <th>VolatileAcidity</th>\n",
              "      <th>CitricAcid</th>\n",
              "      <th>ResidualSugar</th>\n",
              "      <th>Chlorides</th>\n",
              "      <th>FreeSulfurDioxide</th>\n",
              "      <th>TotalSulfurDioxide</th>\n",
              "      <th>Density</th>\n",
              "      <th>pH</th>\n",
              "      <th>Sulphates</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>LabelAppeal</th>\n",
              "      <th>AcidIndex</th>\n",
              "      <th>STARS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.160</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>54.2</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>NaN</td>\n",
              "      <td>268.0</td>\n",
              "      <td>0.99280</td>\n",
              "      <td>3.33</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.160</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>26.1</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-327.0</td>\n",
              "      <td>1.02792</td>\n",
              "      <td>3.38</td>\n",
              "      <td>0.70</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.640</td>\n",
              "      <td>-0.88</td>\n",
              "      <td>14.8</td>\n",
              "      <td>0.037</td>\n",
              "      <td>214.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>0.99518</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.48</td>\n",
              "      <td>22.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>8</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.04</td>\n",
              "      <td>18.8</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>22.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>0.99640</td>\n",
              "      <td>2.24</td>\n",
              "      <td>1.83</td>\n",
              "      <td>6.2</td>\n",
              "      <td>-1</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.330</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>9.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-167.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>0.99457</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1.77</td>\n",
              "      <td>13.7</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   INDEX  TARGET  FixedAcidity  ...  LabelAppeal  AcidIndex  STARS\n",
              "0      1       3           3.2  ...            0          8    2.0\n",
              "1      2       3           4.5  ...           -1          7    3.0\n",
              "2      4       5           7.1  ...           -1          8    3.0\n",
              "3      5       3           5.7  ...           -1          6    1.0\n",
              "4      6       4           8.0  ...            0          9    2.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkSzgL9vRfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b388996c-ad90-4955-e3e5-94153594e09a"
      },
      "source": [
        "# how many observations are contained within the example data set?\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12795"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RcVEDEK7arC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6ed77836-6b58-4550-bd8a-6ddde9ae8ca9"
      },
      "source": [
        "# check for missing values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "INDEX                    0\n",
              "TARGET                   0\n",
              "FixedAcidity             0\n",
              "VolatileAcidity          0\n",
              "CitricAcid               0\n",
              "ResidualSugar          616\n",
              "Chlorides              638\n",
              "FreeSulfurDioxide      647\n",
              "TotalSulfurDioxide     682\n",
              "Density                  0\n",
              "pH                     395\n",
              "Sulphates             1210\n",
              "Alcohol                653\n",
              "LabelAppeal              0\n",
              "AcidIndex                0\n",
              "STARS                 3359\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Dg-Tif7nIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fb9ea9a-fc76-42a4-a528-6aee8c5bcf95"
      },
      "source": [
        "#it's not suggested we do this in general, and we will learn other (better) ways to deal with this\n",
        "#but as a first step, we will drop our missing data\n",
        "df = df.dropna(how='any',axis=0) \n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olQvvWrn8OT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set up an X and y\n",
        "#we make sure to keep everything as a data frame for simplicity\n",
        "y = df[['TARGET']].copy()\n",
        "X = df.drop('TARGET', axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnUFt-qw83pB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d7dc782d-e887-490d-f21f-91ee5b70f0ac"
      },
      "source": [
        "# Now split the data into training and testing subsets. \n",
        "# We'll set aside 30% of the data for testing purposes; Remember to make sure you specify a value for the inital random_state\n",
        "# if you want to have the ability to reproduce the exact same training + testing subsets repeatedly\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
        "print(\"X_train size: \" ,len(X_train))\n",
        "print(\"X_test size: \", len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train size:  4505\n",
            "X_test size:  1931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JmEl2mr9QVl",
        "colab_type": "text"
      },
      "source": [
        "We first separated the response variable from the explanatory variables and then used the train_test_split() function. We have set aside 30% of the data for testing purposes.\n",
        "\n",
        "__NOTE__: We opted to explicitly make a copy of the original data set and delete the response variable from that copy. However, an alternative approach would be to simply create a new dataframe object containing only those attributes you plan to make use of as explanatory variables within your model. There is no need to explicitly create a copy of the entire data set every time you want to split your data into training + testing subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ3i00T7_M_r",
        "colab_type": "text"
      },
      "source": [
        "## __Cross Validation__\n",
        "\n",
        "Cross-validation uses __resampling__ of training data to evaluate the performance of machine learning models on a limited data sample. This allows us to avoid the need for the creation of a distinct evaluation / validation subset.\n",
        "\n",
        "\n",
        "The most common approach to cross validation works as follows:\n",
        "\n",
        "\n",
        "- __Step 1__: Split the training data into K non-randomly sampled subsets. These subsets are referred to as \"__folds__\". \n",
        "\n",
        "\n",
        "- __Step 2__: The model performance is then trained using __K-1__ of the folds as inputs to the training process. The fold not used for model training is used to evaluate the performance of the model. \n",
        "\n",
        "\n",
        "- __Step 3__: Model performance metrics are recorded.\n",
        "\n",
        "\n",
        "- __Step 4__: A different fold is then selected for use as the validation subset\n",
        "\n",
        "\n",
        "- __Step 5__: Repeat Steps 2 through 4 until each of the \"K\" folds has been used as the validation subset. At that point you will have trained and evaluated the model \"K\" number of times on \"K\" different subsets of the training data.\n",
        "\n",
        "\n",
        "This process, referred to as \"__K-Fold Cross Validation__\".\n",
        "\n",
        "\n",
        "This process ca be automated using the __cross_val_score()__ function contained within the scikit-learn library.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What K should we choose?\n",
        "\n",
        "1. The value for \"K\" is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
        "\n",
        "\n",
        "2. K = 10 has been found to have low bias and modest variance.\n",
        "\n",
        "\n",
        "3. \"K\" = n, with n = the number of observations in the training data. This gives each item in the training data set an opportunity to be used as the model validation dataset. This approach is called \"__leave-one-out cross-validation__\" (LOOCV).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Variations of K-Fold Cross Validation\n",
        "\n",
        "- __Stratified Cross Validation__: In stratified cross validation we split the data into folds based on user-specified criteria. A common use of this technique is ensuring that each fold has the same proportion of observations having a given categorical value, e.g., such as porportional number of samples.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "__**** IMPORTANT ****__: When using cross validation __you must still split your data set into training and testing subsets__. Cross validation eliminates the need to create a separate evaluation / validation subset but it __DOES NOT__ eliminate the need for a dedicated model testing subset.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Using scikit-learn's Cross Validation Capabilities: An Example\n",
        "\n",
        "scikit-learn provides us with __cross_val_score()__. The user must first split their data into training and testing subsets (as above) and select an appropriate machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyeDtbpiA3Jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "98d219d5-f362-48c5-b400-841cdafb6877"
      },
      "source": [
        "# load the LinearRegression() function from sklearn's 'linear_model' sub-library\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# load the cross_val_score function from the sklearn.model_selection module\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Assing the model function you want to use to a variable\n",
        "model = LinearRegression()\n",
        "\n",
        "# fit the model using 10-fold cross validation; note how the 'model' variable created above is used as a parameter for the \n",
        "# cross_val_score() function. Also note how we can specify the number of folds to use during cross validation via the 'cv' \n",
        "# parameter\n",
        "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
        "\n",
        "# print out the accuracy metrics derived from the K-fold cross validation process\n",
        "print (scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.37391588 0.4815027  0.46241032 0.38677289 0.39322987 0.46292859\n",
            " 0.44879197 0.44783892 0.42601647 0.41181553]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPdacUeBSFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "542970a0-b4b0-423d-b9aa-b333ccb9b935"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# calculate the average accuracy across all 10 folds\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4295223153950943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l--rTmh2annW",
        "colab_type": "text"
      },
      "source": [
        "We have a very weak performance here of ~43%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jy4X2FmaOZv",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# __Assignment 1__\n",
        "\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZgMG-WXaZCA",
        "colab_type": "text"
      },
      "source": [
        "Cross validation can be applied during model training to assess the performance of a model to assess how well it will handle previously unseen data. \n",
        "\n",
        "We will construct a cross validated linear regression model that predicts the energy production of a power plant. The data set is sourced from the UC Irvine machine learning archive: \n",
        "- https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant# \n",
        "\n",
        "The data set is comprised nearly 10,000 observations of 1 response/dependent variable (net hourly electrical energy output) and 4 explanatory/independent variables (temperature, ambient pressure, relative humidity, and exhaust vacuum)\n",
        "\n",
        "1. Load the provided into your Github Repository. Data Link: [here](https://docs.google.com/spreadsheets/d/1G9434EXtmv6sqtV_A_t8V3TUJspaSVPp7AguQnA7CJQ/edit?usp=sharing) \n",
        "2. Load the data into a pandas dataframe\n",
        "3. Get familiar with the data - this means possibly getting expert domain knowledge\n",
        "4. Do an EDA. Include any important analytical highlights.  Identify preliminary predictive inferences. Make sure to be both thorough and succint. Do not include graphics in your EDA that you do not provide a written explanatory narrative for. Graphics lacking explanatory narratives are of no use to a reader of your work.\n",
        "\n",
        "5. Create two different linear regression models that predict net \n",
        "hourly electrical energy output and evaluate them using K-fold cross validation. \n",
        "\n",
        "6. Upload your final product to Github\n"
      ]
    }
  ]
}
