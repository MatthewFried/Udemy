{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day3_Review.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BzjcrxRMMZD"
      },
      "source": [
        "# Day3 Review\n",
        "\n",
        "\n",
        "## How to approach this (or analogous) problem(s):\n",
        "\n",
        "1) __EDA__: \n",
        "* Calculate correlation metrics amongst all variables\n",
        "* Look for potential collinearity amongst explanatory variables\n",
        "* Examine distributions + summary statistics of all variables + comment on their appearance\n",
        "* Plot relationships between each potential explanatory variable and the response variable\n",
        "* Derive inferences\n",
        "* Identify (and impute) missing values\n",
        "\n",
        "2) __Data Preparation__: \n",
        "* Check which imputation method will be most effective \n",
        "* Check if the data is normal and decide how to handle it if it is not. (Normalization or standardization)\n",
        "\n",
        "3) __Dimensionality Reduction + Feature Selection__: \n",
        "* Based on the results of your EDA, select the explanatory variables you believe are likely to be the most useful within your model(s). \n",
        "* Exclude variables that are highly correlated with one another (i.e., \"collinear\"). \n",
        "* Exclude variables that exhibit low variance. \n",
        "* Consider excluding variables that exhibit little relation with the response variable. \n",
        "* Do all of this __before__ use of PCA or recursive feature elimination or VIF's or p-value analysis. \n",
        "\n",
        "## Potential Models:\n",
        "\n",
        "- Apply PCA to continuous numeric data; select some number of PC's; use selected PC's as the basis of a regression model (excluding all categorical explanatory variables).\n",
        "\n",
        "- Apply PCA to continuous numeric data; select some number of PC's; use selected PC's + a subset categorical variables as the basis of a regression model. Refine model via use of backward and bi-directional selection and VIFs.\n",
        "\n",
        "- Use correlation thresholds, then recursive feature elimination (p-value analysis), then VIFS to produce a model with a small number of statistically significant and non-collinear variables\n",
        "\n",
        "\n",
        "## Use the results of your EDA as the starting point for all downstream work\n",
        "\n",
        "When asked to apply dimensionality reduction and/or feature selection methods to a data set, we should rely upon the results of our EDA work, i.e., our starting point should be the use of the correlation metrics and preliminary predictive inferences we've derived from the data.\n",
        "\n",
        "We should __NOT__ start by simply throwing all of the data we've been given into a backward selection process or PCA: why bother with an EDA if you are simply going to ignore its results? The results of a thorough EDA will typically allow us to construct our models in a much more efficient and effective manner than will simply throwing our hands up and arbitrarily forcing all of our data into a model.\n",
        "\n",
        "## Avoid the use of Python-based tools that you don't fully understand\n",
        "\n",
        "While Python (and many other languages) often provide very simple + highly abstracted tools that enable the implementation of very complex concepts via a very small amount of Python code, we should avoid the use of tools we don't fully understand. Improper use of such highly abstracted tools without sufficient knowledge of their underlying algorithms can easily result in our work being compromised by inaccurate and/or irrelevant results/output while also requiring the computation of large amounts of potentially unnecessary calculations.\n",
        "\n",
        "\n",
        "## PCA\n",
        "\n",
        "__Can we apply PCA to categorical features that have been converted to nominal numeric values (e.g., via one-hot encoding or label enconding)?__\n",
        "\n",
        "The answer is __NO__, we should __NEVER__ apply PCA to __ANY__ type of categorical data, even if the categorical information has been converted to (or was always in) numerical format. __PCA__ is meant to be __applied to continuous variables__, for which it tries to maximize the variance (i.e, the squared deviations) of the data. The concept of squared deviations doesn't really exist when applied to binary or label encoded data. \n",
        "\n",
        "By contrast, __categorical data is measured on a nominal scale__ meaning that the category spacing has no interval/ratio meaning. For example, consider the \"symboling\" discussed previously: \"symboling\" is an __ordinal categorical variable__ having possible values of (-3, -2,..,2, 3). __No meaningful mathematical result can be derived from the addition, subtraction, multiplication, or division of a categorical variable's possible data values__ since the nominally numeric values __are not cardinal numbers__. So while we can say that a \"symboling\" value of -3 is preferable to a \"symboling\" value of 2, we cannot derive meaning from the addition or subtraction these nominal values.\n",
        "\n",
        "So while you can obtain an output from a PCA algorithm based on numeric encodings of categorical inputs, the output is highly unlikely to have any relevant \"meaning\" (i.e., garbage in ... garbage out).\n",
        "\n",
        "_Although_ not everyone agrees, and there are other options:\n",
        "1. [CATPCA](https://www.ibm.com/support/knowledgecenter/en/SSLVMB_23.0.0/spss/categories/idh_cpca.html)\n",
        "2. [Another approach](https://people.orie.cornell.edu/mru8/doc/udell15_pca_dataframe.pdf)\n",
        "3. [Yet another approach](https://arxiv.org/pdf/1410.7404.pdf)\n",
        "4. [And yet another approach](http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf) \n",
        "\n",
        "__Remove highly correlated features prior to PCA__\n",
        "\n",
        "Retaining highly correlated features can cause PCA to __over-emphasize__ the contribution of the highly correlated variables within the principal components + potentially change the direction of the associated eigenvectors and/or the magnitude of the associated eigenvalues. Here's a link to a fairly good explanation of this phenomena [link](https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca)\n",
        "\n",
        "So the answer to the question is __YES__, we should attempt to remove features that appear to be highly correlated with one another prior to applying PCA to a set of continuous numeric data.\n",
        "\n",
        "\n",
        "## Variance Inflation Factors\n",
        "\n",
        "Variance inflation factors (VIFs) are an __OUTPUT__ of __a series of regression models__. To calculate VIFs for a set of explanatory variables, we need to regress every explanatory variable against every other possible explanatory variable. That's $N * (N-1)$ regression models !!!\n",
        "\n",
        "Therefore, we should __NOT__ be using VIFs __before__ we've attempted to remove highly correlated explanatory variables from a data set. VIFs are appropriately derived __from the output of regression models we have constructed using the knowledge we've gained from our EDA work__. This avoids the use of many arbitrary + unnecessary calculations while also __contextualizing the VIFs relative to a model that has been constructed via a process informed inquiry__, as opposed to an arbitrary calculation of VIFs prior to the application of the domain knowledge we develop via an EDA process. "
      ]
    }
  ]
}